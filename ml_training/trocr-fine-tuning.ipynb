{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 14495532,
     "sourceType": "datasetVersion",
     "datasetId": 9220946
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path(\".\")\n",
    "\n",
    "\n",
    "def check_env() -> str:\n",
    "    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE'):\n",
    "        print(\"Running on Kaggle\")\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        print(\"Running locally\")\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = check_env()\n",
    "\n",
    "if ENV == \"kaggle\":\n",
    "    data_dir = Path(\"/kaggle/input/ka-ocr\")\n",
    "else:\n",
    "    data_dir = BASE_DIR / \"data\"\n",
    "\n",
    "print(f\"\\nDataset contents in {data_dir}:\")\n",
    "for item in data_dir.iterdir():\n",
    "    print(f\"{item.name}\")"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:25.878146Z",
     "iopub.execute_input": "2026-01-20T13:11:25.878780Z",
     "iopub.status.idle": "2026-01-20T13:11:25.925964Z",
     "shell.execute_reply.started": "2026-01-20T13:11:25.878749Z",
     "shell.execute_reply": "2026-01-20T13:11:25.924898Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:09.346660Z",
     "start_time": "2026-01-23T18:56:09.338928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n",
      "\n",
      "Dataset contents in data:\n",
      ".cache\n",
      "3d_unicode\n",
      "alkroundedmtav-medium\n",
      "alkroundednusx-medium\n",
      "ar-archy-regular\n",
      "arial_geo\n",
      "arial_geo-bold\n",
      "arial_geo-bold-italic\n",
      "arial_geo-italic\n",
      "bpg_algeti\n",
      "bpg_algeti_compact\n",
      "bpg_arial_2009\n",
      "bpg_boxo\n",
      "bpg_boxo-boxo\n",
      "bpg_classic_medium\n",
      "bpg_dedaena\n",
      "bpg_dedaena_nonblock\n",
      "bpg_excelsior_caps_dejavu_2010\n",
      "bpg_excelsior_dejavu_2010\n",
      "bpg_extrasquare_2009\n",
      "bpg_extrasquare_mtavruli_2009\n",
      "bpg_glaho\n",
      "bpg_glaho_2008\n",
      "bpg_glaho_arial\n",
      "bpg_glaho_bold\n",
      "bpg_glaho_sylfaen\n",
      "bpg_glaho_traditional\n",
      "bpg_ingiri_2008\n",
      "bpg_irubaqidze\n",
      "bpg_mrgvlovani_caps_2010\n",
      "bpg_nino_elite_exp\n",
      "bpg_nino_elite_ultra\n",
      "bpg_nino_elite_ultra_caps\n",
      "bpg_nino_medium_caps\n",
      "bpg_nino_mtavruli_bold\n",
      "bpg_nino_mtavruli_book\n",
      "bpg_nino_mtavruli_normal\n",
      "bpg_no9\n",
      "bpg_nostalgia\n",
      "bpg_paata\n",
      "bpg_paata_caps\n",
      "bpg_paata_cond\n",
      "bpg_paata_cond_caps\n",
      "bpg_paata_exp\n",
      "bpg_phone_sans_bold\n",
      "bpg_phone_sans_bold_italic\n",
      "bpg_phone_sans_italic\n",
      "bpg_quadrosquare_2009\n",
      "bpg_rioni\n",
      "bpg_rioni_contrast\n",
      "bpg_rioni_vera\n",
      "bpg_sans_2008\n",
      "bpg_serif_2008\n",
      "bpg_square_2009\n",
      "bpg_supersquare_2009\n",
      "bpg_ucnobi\n",
      "bpg_venuri_2010\n",
      "fixedsys_excelsior\n",
      "gf_aisi_nus-bold-italic\n",
      "gf_aisi_nus_medium-medium-italic\n",
      "gugeshashvili_slfn_2\n",
      "ka_literaturuli\n",
      "ka_lortkipanidze\n",
      "literaturulitt\n",
      "metadata.csv\n",
      "mg_bitneon\n",
      "mg_bitneon_chaos\n",
      "mg_niniko\n",
      "NotoSansGeorgian\n",
      "version.txt\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": "# Explore data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:25.928197Z",
     "iopub.execute_input": "2026-01-20T13:11:25.928975Z",
     "iopub.status.idle": "2026-01-20T13:11:27.500790Z",
     "shell.execute_reply.started": "2026-01-20T13:11:25.928934Z",
     "shell.execute_reply": "2026-01-20T13:11:27.499473Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:09.415841Z",
     "start_time": "2026-01-23T18:56:09.411884Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv(data_dir/\"metadata.csv\")\nprint(df.head())\nprint(df.tail())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.502627Z",
     "iopub.execute_input": "2026-01-20T13:11:27.503207Z",
     "iopub.status.idle": "2026-01-20T13:11:27.802472Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.503165Z",
     "shell.execute_reply": "2026-01-20T13:11:27.801217Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:09.595294Z",
     "start_time": "2026-01-23T18:56:09.432612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        file_name      text\n",
      "0  3d_unicode/3d_unicode_0000.png        რა\n",
      "1  3d_unicode/3d_unicode_0001.png  წყდებოდა\n",
      "2  3d_unicode/3d_unicode_0002.png   ჭჯნწფუქ\n",
      "3  3d_unicode/3d_unicode_0003.png    წმინდა\n",
      "4  3d_unicode/3d_unicode_0004.png     ჯილდო\n",
      "                                         file_name         text\n",
      "100495  NotoSansGeorgian/NotoSansGeorgian_1495.png     ხთვჰჟშხშ\n",
      "100496  NotoSansGeorgian/NotoSansGeorgian_1496.png       რომლის\n",
      "100497  NotoSansGeorgian/NotoSansGeorgian_1497.png  შემორჩენილი\n",
      "100498  NotoSansGeorgian/NotoSansGeorgian_1498.png    ნეოლითური\n",
      "100499  NotoSansGeorgian/NotoSansGeorgian_1499.png         უცხო\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "print(df[\"text\"].value_counts())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.803840Z",
     "iopub.execute_input": "2026-01-20T13:11:27.804224Z",
     "iopub.status.idle": "2026-01-20T13:11:27.848198Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.804188Z",
     "shell.execute_reply": "2026-01-20T13:11:27.846856Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:09.640649Z",
     "start_time": "2026-01-23T18:56:09.612085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "და              4205\n",
      "არ              1155\n",
      "რომ             1044\n",
      "იყო              785\n",
      "კი               612\n",
      "                ... \n",
      "ფოდჩ               1\n",
      "ვჯნც               1\n",
      "ტემპი              1\n",
      "ჰიბიწნჟცჭჟქი       1\n",
      "ჯპჭოჯგ             1\n",
      "Name: count, Length: 37994, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": "# Check text length variations\ndf[\"text_len\"] = df[\"text\"].str.len()\nprint(df[\"text_len\"].describe())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.850754Z",
     "iopub.execute_input": "2026-01-20T13:11:27.851014Z",
     "iopub.status.idle": "2026-01-20T13:11:27.932375Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.850982Z",
     "shell.execute_reply": "2026-01-20T13:11:27.931260Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:09.721697Z",
     "start_time": "2026-01-23T18:56:09.679014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    100500.000000\n",
      "mean          6.396020\n",
      "std           2.979581\n",
      "min           2.000000\n",
      "25%           4.000000\n",
      "50%           6.000000\n",
      "75%           8.000000\n",
      "max          30.000000\n",
      "Name: text_len, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "source": "# Prepare dataset and tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Checking if trocr already support tokenization for Georgian",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from transformers import TrOCRProcessor\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n\n# Test Georgian tokenization\ntest_text = \"გამარჯობა\"\ntokens = processor.tokenizer.tokenize(test_text)\nprint(tokens)  # If you see lots of <unk> or weird splits, you need a custom tokenizer",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:11:27.933770Z",
     "iopub.execute_input": "2026-01-20T13:11:27.934453Z",
     "iopub.status.idle": "2026-01-20T13:12:07.687405Z",
     "shell.execute_reply.started": "2026-01-20T13:11:27.934416Z",
     "shell.execute_reply": "2026-01-20T13:12:07.686212Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:11.988100Z",
     "start_time": "2026-01-23T18:56:09.738192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['á', 'ĥ', 'Ĵ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'Ľ', 'á', 'ĥ', 'Ĳ', 'á', 'ĥ', 'ł', 'á', 'ĥ', '¯', 'á', 'ĥ', 'Ŀ', 'á', 'ĥ', 'ĳ', 'á', 'ĥ', 'Ĳ']\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's not what we need, so we'll create custom, character-based tokenizer.\n",
    "\n",
    "The model predicts next token, in this case token represents char, not a word."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class GeorgianTokenizer:\n    def __init__(self, max_length: int = 32):\n        # Special tokens\n        self.pad_token = \"<pad>\"\n        self.bos_token = \"<s>\"      # beginning of sequence\n        self.eos_token = \"</s>\"     # end of sequence\n        self.unk_token = \"<unk>\"    # unknown character\n\n        # Georgian alphabet (33 letters)\n        self.georgian_chars = \"აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ\"\n\n        # Build vocabulary: special tokens + Georgian characters\n        self.vocab = [self.pad_token, self.bos_token, self.eos_token, self.unk_token]\n        self.vocab.extend(list(self.georgian_chars))\n\n        # Create mappings\n        self.char_to_id = {char: idx for idx, char in enumerate(self.vocab)}\n        self.id_to_char = {idx: char for idx, char in enumerate(self.vocab)}\n\n        # Token IDs for special tokens\n        self.pad_token_id = 0\n        self.bos_token_id = 1\n        self.eos_token_id = 2\n        self.unk_token_id = 3\n\n        self.max_length = max_length\n\n    def encode(self, text: str, padding: bool = True) -> list[int]:\n        \"\"\"Convert Georgian text to token IDs.\"\"\"\n        # Start with BOS token\n        ids = [self.bos_token_id]\n\n        # Convert each character\n        for char in text:\n            ids.append(self.char_to_id.get(char, self.unk_token_id))\n\n        # Add EOS token\n        ids.append(self.eos_token_id)\n\n        # Truncate if too long\n        if len(ids) > self.max_length:\n            ids = ids[:self.max_length - 1] + [self.eos_token_id]\n\n        # Pad if needed\n        if padding:\n            ids.extend([self.pad_token_id] * (self.max_length - len(ids)))\n\n        return ids\n\n    def decode(self, ids: list[int]) -> str:\n        \"\"\"Convert token IDs back to text.\"\"\"\n        chars = []\n        for id in ids:\n            if id in (self.pad_token_id, self.bos_token_id, self.eos_token_id):\n                continue\n            chars.append(self.id_to_char.get(id, \"\"))\n        return \"\".join(chars)\n\n    def __len__(self):\n        return len(self.vocab)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.689057Z",
     "iopub.execute_input": "2026-01-20T13:12:07.689789Z",
     "iopub.status.idle": "2026-01-20T13:12:07.702648Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.689756Z",
     "shell.execute_reply": "2026-01-20T13:12:07.701223Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:12.196593Z",
     "start_time": "2026-01-23T18:56:12.189134Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "source": "Test tokenization again",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "tokenizer = GeorgianTokenizer(max_length=32)\n\n# Test encoding\ntext = \"გამარჯობა\"\nids = tokenizer.encode(text)\nprint(f\"Text: {text}\")\nprint(f\"IDs: {ids[:15]}...\")  # First 15 tokens\nprint(f\"Length: {len(ids)}\")\n\n# Test decoding\ndecoded = tokenizer.decode(ids)\nprint(f\"Decoded: {decoded}\")\n\n# Verify vocab size\nprint(f\"Vocab size: {len(tokenizer)}\")  # Should be 37 (4 special + 33 Georgian)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.704028Z",
     "iopub.execute_input": "2026-01-20T13:12:07.704377Z",
     "iopub.status.idle": "2026-01-20T13:12:07.741194Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.704351Z",
     "shell.execute_reply": "2026-01-20T13:12:07.739725Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:12.212292Z",
     "start_time": "2026-01-23T18:56:12.205638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: გამარჯობა\n",
      "IDs: [1, 6, 4, 15, 4, 20, 35, 17, 5, 4, 2, 0, 0, 0, 0]...\n",
      "Length: 32\n",
      "Decoded: გამარჯობა\n",
      "Vocab size: 37\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "source": "Works as expected.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Now we prepare dataset using this tokenizer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image, ImageOps\n\n\nclass GeorgianOCRDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root_dir: str, processor, tokenizer: GeorgianTokenizer):\n        self.df = df.reset_index(drop=True)\n        self.root_dir = root_dir\n        self.processor = processor\n        self.tokenizer = tokenizer  # custom tokenizer\n\n    def __len__(self) -> int:\n        return len(self.df)\n\n    def __getitem__(self, idx: int) -> dict[str, torch.Tensor]:\n        text = self.df.iloc[idx]['text']\n        file_path = f\"{self.root_dir}/{self.df.iloc[idx]['file_name']}\"\n\n        # Open and process image\n        img = Image.open(file_path).convert(\"RGB\")\n        w, h = img.size\n        target_size = 384\n\n        # Scale height to target_size, width proportionally\n        scale = target_size / max(w, h)\n        new_w, new_h = int(w * scale), int(h * scale)\n        img = img.resize((new_w, new_h), Image.Resampling.BILINEAR)\n\n        # Pad to square\n        new_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n        offset = ((target_size - new_w) // 2, (target_size - new_h) // 2)\n        new_img.paste(img, offset)\n\n        # Use Processor for Normalization\n        pixel_values = self.processor(new_img, return_tensors=\"pt\").pixel_values\n\n        # Tokenize Georgian Text\n        labels = self.tokenizer.encode(text)\n\n        # Replace padding token id with -100 so it's ignored by the loss function\n        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n\n        return {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"labels\": torch.tensor(labels)\n        }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.742793Z",
     "iopub.execute_input": "2026-01-20T13:12:07.743248Z",
     "iopub.status.idle": "2026-01-20T13:12:07.767293Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.743205Z",
     "shell.execute_reply": "2026-01-20T13:12:07.766182Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:12.251528Z",
     "start_time": "2026-01-23T18:56:12.243509Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": "Prepare model...\n\nwe give it our 37 tokens so model predicts only 37 possible outputs instead of original 50k.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "# Create your tokenizer\n",
    "tokenizer = GeorgianTokenizer(max_length=32)\n",
    "\n",
    "# Load model and resize token embeddings\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))  # Resize to 37\n",
    "\n",
    "# Configure special tokens\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:12:07.768621Z",
     "iopub.execute_input": "2026-01-20T13:12:07.769192Z",
     "iopub.status.idle": "2026-01-20T13:12:15.417654Z",
     "shell.execute_reply.started": "2026-01-20T13:12:07.769142Z",
     "shell.execute_reply": "2026-01-20T13:12:15.416666Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:13.496874Z",
     "start_time": "2026-01-23T18:56:12.282557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "source": "# Train test split",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\n\ntrain_df, test_df = train_test_split(\n    df, \n    test_size=0.10, \n    random_state=42, \n    shuffle=True\n)\n\nprint(train_df[\"text\"].value_counts())",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2026-01-20T13:37:44.097814Z",
     "iopub.execute_input": "2026-01-20T13:37:44.098998Z",
     "iopub.status.idle": "2026-01-20T13:37:44.154832Z",
     "shell.execute_reply.started": "2026-01-20T13:37:44.098952Z",
     "shell.execute_reply": "2026-01-20T13:37:44.153900Z"
    },
    "ExecuteTime": {
     "end_time": "2026-01-23T18:56:13.743231Z",
     "start_time": "2026-01-23T18:56:13.694601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "და               3803\n",
      "არ               1044\n",
      "რომ               947\n",
      "იყო               705\n",
      "კი                546\n",
      "                 ... \n",
      "სლგწრნუძწასდ        1\n",
      "ლბევწდც             1\n",
      "+995549397648       1\n",
      "სიამოვნებასაც       1\n",
      "ზელენსკის           1\n",
      "Name: count, Length: 35172, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataloaders"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T19:05:32.492824Z",
     "start_time": "2026-01-23T19:05:32.456745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = GeorgianOCRDataset(train_df, data_dir, processor, tokenizer)\n",
    "test_dataset = GeorgianOCRDataset(test_df, data_dir, processor, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 11307, Test batches: 1257\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Set up training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T19:06:09.184932Z",
     "start_time": "2026-01-23T19:06:09.151301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "print(f\"Training on: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: cpu\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training loop"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} complete. Avg Loss: {avg_loss:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Saving the fine-tuned model"
  }
 ]
}
